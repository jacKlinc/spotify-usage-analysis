{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artist Prediction Model ðŸŽ¶\n",
    "\n",
    "This model attempts to predict a user's preferred artist based on their top 50 artists from Spotify. The information is limited, but hopefully something useful might come of it.\n",
    "\n",
    "### Eric Boam's Analysis\n",
    "Eric collected data on music from Man, Machine and Media. Media being friends on social media; this saw the highest percentage of top ten albums (highest quality) while the Machine had a lower percentage but many more data points (highest quantity).\n",
    "\n",
    "The Release Radar playlist is released daily for each user. \n",
    "These are the general trends that Eric found:\n",
    "- Songs added at midnight\n",
    "- Keep song on list for max 4 weeks\n",
    "- Favour familiar artists\n",
    "- Use remixes when music runs thin\n",
    "\n",
    "[More on this](https://medium.com/@ericboam/i-decoded-the-spotify-recommendation-algorithm-heres-what-i-found-4b0f3654035b)\n",
    "\n",
    "### BaRT\n",
    "The AI system in charge of organising your Spotify homepage is called Bandits for Recommendations as Treatments (BaRT). It works on the balance between exploit and explore, the former recommendinf what the user already likes and the latter showing them new stuff.\n",
    "Important metrics:\n",
    "1. 30-Sec rule-ignore songs that get skipped \n",
    "2. Audio Analysis-idenitfy audio patterns to find more like it\n",
    "3. User age, location and gender-find what others like\n",
    "\n",
    "[More on this](https://analyticsindiamag.com/how-spotifys-algorithm-manages-to-find-your-inner-groove/)\n",
    "\n",
    "#### 30-Sec Rule:\n",
    "\n",
    "This generally indicates that someone likes a song, listening to the end should further add to this. Continuously listening to a song over a period further indeicates appeal as it prevents the \"One Hit Wonder Issue\" which could skew recommendations with trending songs.\n",
    "\n",
    "\n",
    "#### Artist/Song Follow-Up\n",
    "\n",
    "When a user listens to a song, do they checkout the artist after? Are their next played songs from that artist/album? \n",
    "\n",
    "These are called \"high-quality actions\" where these actions give a very good indication of the user liking it.\n",
    "\n",
    "\n",
    "### Recommendation Models\n",
    "1. **Collaborative Filtering**\n",
    "\n",
    "    Layperson:\n",
    "        This finds other users who like the same stuff as you and recommends what you don't have\n",
    "\n",
    "\n",
    "    \n",
    "[More on this](https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe)\n",
    "    \n",
    "\n",
    "2. **Acoustic Analysis**\n",
    "    \n",
    "    Spotify gives the high-level audio characteristics that describe how the song sounds. Some include \"danceability\" and \"tempo\".\n",
    "\n",
    "    Sample JSON Response:\n",
    "    ``` json\n",
    "    {\n",
    "      \"duration_ms\" : 255349,\n",
    "      \"key\" : 5,\n",
    "      \"mode\" : 0,\n",
    "      \"time_signature\" : 4,\n",
    "      \"acousticness\" : 0.514,\n",
    "      \"danceability\" : 0.735,\n",
    "      \"energy\" : 0.578,\n",
    "      \"instrumentalness\" : 0.0902,\n",
    "      \"liveness\" : 0.159,\n",
    "      \"loudness\" : -11.840,\n",
    "      \"speechiness\" : 0.0461,\n",
    "      \"valence\" : 0.624,\n",
    "      \"tempo\" : 98.002,\n",
    "      \"id\" : \"06AKEBrKUckW0KREUWRnvT\",\n",
    "      \"uri\" : \"spotify:track:06AKEBrKUckW0KREUWRnvT\",\n",
    "      \"track_href\" : \"https://api.spotify.com/v1/tracks/06AKEBrKUckW0KREUWRnvT\",\n",
    "      \"analysis_url\" : \"https://api.spotify.com/v1/audio-analysis/06AKEBrKUckW0KREUWRnvT\",\n",
    "      \"type\" : \"audio_features\"\n",
    "    }\n",
    "    ```\n",
    "    The above fields are a lot easier to work with compared to raw audio data which is difficult to analyse and expensive to process.\n",
    "    \n",
    "    \n",
    "    \n",
    "3. **Natural Language Processing**\n",
    "\n",
    "    Spotify web scrapes for similar artists. *Hard to do-exclude.*\n",
    "\n",
    "\n",
    "### Their Data Pipeline\n",
    "The image below is an estimate of the architecture used in Spotify to supply recommendations.\n",
    "\n",
    "![Spotify Data Pipeline](../res/spotify_pipeline.png)\n",
    "\n",
    "The \"Batch Audio Models\" and \"Batch NLP Models\" are little out of my depth and require a lot of processing to get done so Audio/Text Analysis of songs will be left out. The main focus will be:\n",
    "- Play Logs\n",
    "- Track metadata\n",
    "- Batch CF Models (Collabarative Filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Model Columns\n",
    "\n",
    "***Given a user's top 50 artists, recommend 5 other artists***\n",
    "\n",
    "1. Get info for each artist\n",
    "2. Search for artist with similar information on MusicBrainz/Cache\n",
    "3. Find artists not in Top 50\n",
    "\n",
    "The predicted column (y) in this case is the name of the recommended artist. The columns used to predict y are the remaining columns (X).\n",
    "\n",
    "**X Columns: Nationality, Age, Genres, Gender, Race (not done)**\n",
    "\n",
    "- The Model would have to search through the cached JSON file for artists with similar attributes.\n",
    "- A LabelEncoder would have to be implemented to deal with the non-numeric values\n",
    "    - Nationality: 0-194\n",
    "    - Age: 0-90\n",
    "    - Genre: map most popular to 0-x\n",
    "    - Gender: True/False\n",
    "    - Race: map most popular to 0-x (not done)\n",
    "    - Artist: map most popular to 0-10000 \n",
    "    - Type: True/False\n",
    "- More data needs to be gathered for favourite artists\n",
    "- More than the current top 500 artists would need to be collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import spotipy.util as util\n",
    "# from client_secret import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load My Top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>type</th>\n",
       "      <th>country</th>\n",
       "      <th>city_1</th>\n",
       "      <th>district_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>district_2</th>\n",
       "      <th>city_3</th>\n",
       "      <th>district_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drake</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>person</td>\n",
       "      <td>CA</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travis Scott</td>\n",
       "      <td>male</td>\n",
       "      <td>28</td>\n",
       "      <td>person</td>\n",
       "      <td>US</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>Houston</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Juice WRLD</td>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>person</td>\n",
       "      <td>US</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>n/a</td>\n",
       "      <td>Oak Lawn</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>person</td>\n",
       "      <td>CA</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>Scarborough</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Billie Eilish</td>\n",
       "      <td>female</td>\n",
       "      <td>18</td>\n",
       "      <td>person</td>\n",
       "      <td>US</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          artist  gender age    type country city_1 district_1       city_2  \\\n",
       "0          Drake    male  33  person      CA    n/a        n/a      Toronto   \n",
       "1   Travis Scott    male  28  person      US    n/a        n/a      Houston   \n",
       "2     Juice WRLD    male  21  person      US    n/a        n/a      Chicago   \n",
       "3     The Weeknd    male  30  person      CA    n/a        n/a  Scarborough   \n",
       "4  Billie Eilish  female  18  person      US    n/a        n/a  Los Angeles   \n",
       "\n",
       "  district_2    city_3 district_3  \n",
       "0        n/a       n/a        n/a  \n",
       "1        n/a       n/a        n/a  \n",
       "2        n/a  Oak Lawn        n/a  \n",
       "3        n/a       n/a        n/a  \n",
       "4        n/a       n/a        n/a  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9e7c3c4d5660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load My Yop 50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m mytop_50 = pd.read_json('../data/mytop_50.json', dtype={\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m'artist'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'object'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'gender'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'object'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1089\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m             )\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load My Yop 50\n",
    "mytop_50 = pd.read_json('../data/mytop_50.json', dtype={\n",
    "    'artist': 'object',\n",
    "    'gender': 'object',\n",
    "    'age': 'int64',\n",
    "    'type': 'object',\n",
    "    'country': 'object',\n",
    "    'city_1': 'object',\n",
    "    'district_1': 'object',\n",
    "    'city_2': 'object',\n",
    "    'district_2': 'object',\n",
    "    'city_3': 'object',\n",
    "    'district_3': 'object'\n",
    "})\n",
    "\n",
    "# Predicted value\n",
    "y = mytop_50.index\n",
    "\n",
    "\n",
    "# Find 'n/a' columns\n",
    "missing_rows = mytop_50.loc[mytop_50.country == 'n/a']\n",
    "rows = mytop_50.country - missing_rows.country\n",
    "rows\n",
    "\n",
    "# Drop columns\n",
    "# mytop_50.drop(['city_1', 'district_1', 'city_2', 'district_2', 'city_3', 'district_3', 'gender', 'type'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(mytop_50, y,\n",
    "#                                                       train_size=0.8, test_size=0.2,\n",
    "#                                                       random_state=0)\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Mean Absolute Error (MAE) Function\n",
    "This finds the average error of the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode Countries\n",
    "Label encode countries so model can make sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AD</th>\n",
       "      <th>42.546245</th>\n",
       "      <th>1.601554</th>\n",
       "      <th>Andorra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>23.424076</td>\n",
       "      <td>53.847818</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF</td>\n",
       "      <td>33.939110</td>\n",
       "      <td>67.709953</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AG</td>\n",
       "      <td>17.060816</td>\n",
       "      <td>-61.796428</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI</td>\n",
       "      <td>18.220554</td>\n",
       "      <td>-63.068615</td>\n",
       "      <td>Anguilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>41.153332</td>\n",
       "      <td>20.168331</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>YE</td>\n",
       "      <td>15.552727</td>\n",
       "      <td>48.516388</td>\n",
       "      <td>Yemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>YT</td>\n",
       "      <td>-12.827500</td>\n",
       "      <td>45.166244</td>\n",
       "      <td>Mayotte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>ZA</td>\n",
       "      <td>-30.559482</td>\n",
       "      <td>22.937506</td>\n",
       "      <td>South Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>ZM</td>\n",
       "      <td>-13.133897</td>\n",
       "      <td>27.849332</td>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>ZW</td>\n",
       "      <td>-19.015438</td>\n",
       "      <td>29.154857</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AD  42.546245   1.601554               Andorra\n",
       "0    AE  23.424076  53.847818  United Arab Emirates\n",
       "1    AF  33.939110  67.709953           Afghanistan\n",
       "2    AG  17.060816 -61.796428   Antigua and Barbuda\n",
       "3    AI  18.220554 -63.068615              Anguilla\n",
       "4    AL  41.153332  20.168331               Albania\n",
       "..   ..        ...        ...                   ...\n",
       "239  YE  15.552727  48.516388                 Yemen\n",
       "240  YT -12.827500  45.166244               Mayotte\n",
       "241  ZA -30.559482  22.937506          South Africa\n",
       "242  ZM -13.133897  27.849332                Zambia\n",
       "243  ZW -19.015438  29.154857              Zimbabwe\n",
       "\n",
       "[244 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = pd.read_csv('countries.csv')\n",
    "countries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bite837ac3d978644508280cde9286add7f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
